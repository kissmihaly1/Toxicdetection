{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (2.0.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from pandas) (1.24.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: numpy in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (1.24.3)\r\n",
      "Requirement already satisfied: sklearn in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (0.0.post5)\r\n",
      "Requirement already satisfied: tensorflow in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (2.13.0rc1)\r\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0-rc1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow) (2.13.0rc1)\r\n",
      "Requirement already satisfied: packaging in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (23.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (65.6.3)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (1.4.0)\r\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1rc0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (2.13.1rc0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (4.6.3)\r\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (1.24.3)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (23.5.26)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (4.23.2)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (2.3.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (16.0.0)\r\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (2.13.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (1.15.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (1.54.2)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0rc0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (2.13.0rc0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc1->tensorflow) (3.8.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0-rc1->tensorflow) (0.38.4)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (2.3.6)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (2.19.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (2.28.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (0.7.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (1.26.15)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (4.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (5.3.1)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (0.3.0)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (2023.5.7)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (3.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (0.5.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc1->tensorflow) (3.2.2)\r\n",
      "Requirement already satisfied: torch in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (2.0.1)\r\n",
      "Requirement already satisfied: networkx in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from torch) (4.6.3)\r\n",
      "Requirement already satisfied: filelock in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from torch) (3.12.1)\r\n",
      "Requirement already satisfied: sympy in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: transformers in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (4.30.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from transformers) (1.24.3)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from transformers) (0.15.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from transformers) (2023.6.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from transformers) (4.65.0)\r\n",
      "Requirement already satisfied: filelock in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from transformers) (3.12.1)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from transformers) (0.3.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: requests in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from transformers) (2.28.1)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from transformers) (23.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\r\n",
      "Requirement already satisfied: fsspec in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\r\n",
      "Requirement already satisfied: spacy in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (3.5.3)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (1.1.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (2.0.7)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (1.24.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (3.1.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (23.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (6.3.0)\r\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (8.1.10)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (1.0.9)\r\n",
      "Requirement already satisfied: setuptools in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (65.6.3)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (3.0.8)\r\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (0.7.0)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (2.0.8)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (4.65.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (1.10.9)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (3.3.0)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (1.0.4)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (2.28.1)\r\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (0.10.1)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from spacy) (2.4.6)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kissmihaly/miniconda3/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install spacy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kissmihaly/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "from bisect import bisect_left, bisect_right\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import ast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   annotation  comment_id  worker country  all toxic  not toxic\n",
      "0           0     5167187     868     USA      False      False\n",
      "1           1     5167187    1316     USA      False      False\n",
      "2           2     5167187    1295     USA      False       True\n",
      "3           3     5167187    2856     USA      False      False\n",
      "4           4     5521110     418     VEN       True       True\n",
      "   comment_id                                       comment_text\n",
      "0      239607  Yet call out all Muslims for the acts of a few...\n",
      "1      239612  This bitch is nuts. Who would read a book by a...\n",
      "2      240311                                   You're an idiot.\n",
      "3      240400  Nincompoop, that's a nice one! I'm partial to ...\n",
      "4      240461  testing purposes:\\n\\nyou are an idiot and i ca...\n",
      "   annotation    type  start  end\n",
      "0           0  Insult    133  139\n",
      "1           1  Insult     84   92\n",
      "2           1  Insult    118  124\n",
      "3           1  Insult    126  131\n",
      "4           1  Insult    133  147\n"
     ]
    }
   ],
   "source": [
    "df_annotations = pd.read_csv('annotations')\n",
    "\n",
    "df_comments= pd.read_csv('comments')\n",
    "\n",
    "df_spans= pd.read_csv('spans.csv')\n",
    "\n",
    "\n",
    "print(df_annotations.head(5))\n",
    "print(df_comments.head(5))\n",
    "print(df_spans.head(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "merge = pd.merge(df_annotations, df_spans, on='annotation', how='inner')\n",
    "df_all = pd.merge(merge, df_comments, on='comment_id', how='inner')\n",
    "df_all['word']=\"\"\n",
    "df_all=df_all.head(800)\n",
    "\n",
    "for i in range(0, 799):\n",
    "  str1=df_all['comment_text'][i]\n",
    "  str2=\"\"\n",
    "  for j in range(df_all['start'][i],df_all['end'][i]):\n",
    "    if df_all['end'][i]<=len(str1):\n",
    "      str2=str2+str1[j]\n",
    "  df_all.at[i, 'word'] = str2\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "     annotation  comment_id  worker country  all toxic  not toxic  \\\n0             0     5167187     868     USA      False      False   \n1             1     5167187    1316     USA      False      False   \n2             1     5167187    1316     USA      False      False   \n3             1     5167187    1316     USA      False      False   \n4             3     5167187    2856     USA      False      False   \n..          ...         ...     ...     ...        ...        ...   \n588         455      763785    1043     USA      False      False   \n589         456      763785     519     USA      False      False   \n590         457      498684    2585     USA      False      False   \n591         458      498684      67     VEN      False      False   \n592         459      498684    1272     USA       True      False   \n\n                type  start  end  \\\n0             Insult    133  139   \n1             Insult     84   92   \n2             Insult    118  124   \n3             Insult    126  131   \n4             Insult     84   92   \n..               ...    ...  ...   \n588           Insult     32   37   \n589           Insult     32   37   \n590           Insult     88   95   \n591  Profane/Obscene     88   95   \n592           Insult     88   95   \n\n                                          comment_text      word  \n0    That's right. They are not normal. And I am st...    sexist  \n1    That's right. They are not normal. And I am st...  ABNORMAL  \n2    That's right. They are not normal. And I am st...    racist  \n3    That's right. They are not normal. And I am st...     bigot  \n4    That's right. They are not normal. And I am st...  ABNORMAL  \n..                                                 ...       ...  \n588      grin and you are nothing what a moron you are     moron  \n589      grin and you are nothing what a moron you are     moron  \n590  Exactly.  I wonder if paid subscribers to the ...   suckers  \n591  Exactly.  I wonder if paid subscribers to the ...   suckers  \n592  Exactly.  I wonder if paid subscribers to the ...   suckers  \n\n[593 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>annotation</th>\n      <th>comment_id</th>\n      <th>worker</th>\n      <th>country</th>\n      <th>all toxic</th>\n      <th>not toxic</th>\n      <th>type</th>\n      <th>start</th>\n      <th>end</th>\n      <th>comment_text</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>5167187</td>\n      <td>868</td>\n      <td>USA</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Insult</td>\n      <td>133</td>\n      <td>139</td>\n      <td>That's right. They are not normal. And I am st...</td>\n      <td>sexist</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>5167187</td>\n      <td>1316</td>\n      <td>USA</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Insult</td>\n      <td>84</td>\n      <td>92</td>\n      <td>That's right. They are not normal. And I am st...</td>\n      <td>ABNORMAL</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>5167187</td>\n      <td>1316</td>\n      <td>USA</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Insult</td>\n      <td>118</td>\n      <td>124</td>\n      <td>That's right. They are not normal. And I am st...</td>\n      <td>racist</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>5167187</td>\n      <td>1316</td>\n      <td>USA</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Insult</td>\n      <td>126</td>\n      <td>131</td>\n      <td>That's right. They are not normal. And I am st...</td>\n      <td>bigot</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>5167187</td>\n      <td>2856</td>\n      <td>USA</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Insult</td>\n      <td>84</td>\n      <td>92</td>\n      <td>That's right. They are not normal. And I am st...</td>\n      <td>ABNORMAL</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>588</th>\n      <td>455</td>\n      <td>763785</td>\n      <td>1043</td>\n      <td>USA</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Insult</td>\n      <td>32</td>\n      <td>37</td>\n      <td>grin and you are nothing what a moron you are</td>\n      <td>moron</td>\n    </tr>\n    <tr>\n      <th>589</th>\n      <td>456</td>\n      <td>763785</td>\n      <td>519</td>\n      <td>USA</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Insult</td>\n      <td>32</td>\n      <td>37</td>\n      <td>grin and you are nothing what a moron you are</td>\n      <td>moron</td>\n    </tr>\n    <tr>\n      <th>590</th>\n      <td>457</td>\n      <td>498684</td>\n      <td>2585</td>\n      <td>USA</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Insult</td>\n      <td>88</td>\n      <td>95</td>\n      <td>Exactly.  I wonder if paid subscribers to the ...</td>\n      <td>suckers</td>\n    </tr>\n    <tr>\n      <th>591</th>\n      <td>458</td>\n      <td>498684</td>\n      <td>67</td>\n      <td>VEN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Profane/Obscene</td>\n      <td>88</td>\n      <td>95</td>\n      <td>Exactly.  I wonder if paid subscribers to the ...</td>\n      <td>suckers</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>459</td>\n      <td>498684</td>\n      <td>1272</td>\n      <td>USA</td>\n      <td>True</td>\n      <td>False</td>\n      <td>Insult</td>\n      <td>88</td>\n      <td>95</td>\n      <td>Exactly.  I wonder if paid subscribers to the ...</td>\n      <td>suckers</td>\n    </tr>\n  </tbody>\n</table>\n<p>593 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = df_all[df_all['start'] <= df_all['end']]\n",
    "df_all = df_all[df_all['word'].notna() & (df_all['word'] != '')]\n",
    "df_all['word'] = df_all['word'].apply(lambda x: x if len(x.split()) == 1 else None)\n",
    "df_all.dropna(subset=['word'], inplace=True)\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "df_all"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/ipavlopoulos/toxic_spans/master/SemEval2021/data/tsd_test.csv'\n",
    "df_test= pd.read_csv(url)\n",
    "df_test = df_test.rename(columns={df_test.columns[0]: 'index'})\n",
    "df_test[\"toxic\"]=0\n",
    "df_test=df_test.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kissmihaly/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\")\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "for start in range(0, df_test.shape[0], batch_size):\n",
    "    end = min(start + batch_size, df_test.shape[0])\n",
    "    batch = df_test[\"text\"][start:end]\n",
    "\n",
    "    tokenized_sentences = tokenizer.batch_encode_plus(\n",
    "        batch.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = tokenized_sentences[\"input_ids\"]\n",
    "    attention_mask = tokenized_sentences[\"attention_mask\"]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    def checkpoint_forward(model, *inputs):\n",
    "        return model(*inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = model.to(device)\n",
    "        outputs = checkpoint(checkpoint_forward, model, input_ids, attention_mask)\n",
    "\n",
    "    for i in range(start, end):\n",
    "        scores = outputs.logits[i - start].cpu().tolist()\n",
    "        is_toxic = float(np.max(scores) > 0.2)\n",
    "        df_test.at[i, \"toxic\"] = is_toxic\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  index  \\\n0     [84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...   \n1                              [81, 82, 83, 84, 85, 86]   \n2                                                    []   \n3                                                    []   \n4                                                    []   \n...                                                 ...   \n1995  [4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...   \n1996                               [23, 24, 25, 26, 27]   \n1997  [157, 158, 159, 160, 161, 162, 163, 164, 165, ...   \n1998                                                 []   \n1999                               [828, 829, 830, 831]   \n\n                                                   text  toxic  \\\n0     That's right. They are not normal. And I am st...      1   \n1     \"Watch people die from taking away their healt...      1   \n2     tens years ago i contacted the PDR and suggest...      0   \n3     The parallels between the ANC and the Sicilian...      0   \n4     Intel Community: ‘How can we work for a Presid...      0   \n...                                                 ...    ...   \n1995  hey loser change your name to something more a...      1   \n1996  And you are a complete moron who obviously doe...      1   \n1997  Such vitriol from the left.  Who would have th...      1   \n1998  It is now time for most of you to expand your ...      0   \n1999  Why does this author think she can demand, or ...      0   \n\n                                                 pindex  \n0     [118, 119, 120, 121, 122, 123, 126, 127, 128, ...  \n1                              [81, 82, 83, 84, 85, 86]  \n2                                                    []  \n3                                                    []  \n4                                                    []  \n...                                                 ...  \n1995  [4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...  \n1996  [23, 24, 25, 26, 27, 80, 81, 82, 83, 84, 85, 8...  \n1997  [99, 100, 101, 102, 108, 109, 110, 111, 112, 1...  \n1998                                                 []  \n1999                                                 []  \n\n[2000 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>text</th>\n      <th>toxic</th>\n      <th>pindex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...</td>\n      <td>That's right. They are not normal. And I am st...</td>\n      <td>1</td>\n      <td>[118, 119, 120, 121, 122, 123, 126, 127, 128, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[81, 82, 83, 84, 85, 86]</td>\n      <td>\"Watch people die from taking away their healt...</td>\n      <td>1</td>\n      <td>[81, 82, 83, 84, 85, 86]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[]</td>\n      <td>tens years ago i contacted the PDR and suggest...</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[]</td>\n      <td>The parallels between the ANC and the Sicilian...</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[]</td>\n      <td>Intel Community: ‘How can we work for a Presid...</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>[4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...</td>\n      <td>hey loser change your name to something more a...</td>\n      <td>1</td>\n      <td>[4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>[23, 24, 25, 26, 27]</td>\n      <td>And you are a complete moron who obviously doe...</td>\n      <td>1</td>\n      <td>[23, 24, 25, 26, 27, 80, 81, 82, 83, 84, 85, 8...</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>[157, 158, 159, 160, 161, 162, 163, 164, 165, ...</td>\n      <td>Such vitriol from the left.  Who would have th...</td>\n      <td>1</td>\n      <td>[99, 100, 101, 102, 108, 109, 110, 111, 112, 1...</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>[]</td>\n      <td>It is now time for most of you to expand your ...</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>[828, 829, 830, 831]</td>\n      <td>Why does this author think she can demand, or ...</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged F1-score: 0.49886170280268227\n"
     ]
    }
   ],
   "source": [
    "#TOXICWORDS.TXT\n",
    "def predict_txt(path, df):\n",
    "    df_test['pindex']=''\n",
    "    with open(path, \"r\") as file:\n",
    "        toxic_words = [word.strip() for word in file]\n",
    "\n",
    "    for i in range(0, df.shape[0]):\n",
    "        if df['toxic'][i]==1:\n",
    "            sentence = df['text'][i]\n",
    "            indexes = []\n",
    "            for word in toxic_words:\n",
    "                pattern = r\"\\b\" + re.escape(word) + r\"\\b\"\n",
    "                matches = re.finditer(pattern, sentence, flags=re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    start_index = match.start()\n",
    "                    end_index = match.end()\n",
    "                    indexes.extend(list(range(start_index, end_index)))\n",
    "\n",
    "            indexes.sort()\n",
    "            df.at[i, 'pindex'] = indexes\n",
    "        else:\n",
    "            df.at[i, 'pindex'] = []\n",
    "    return df\n",
    "\n",
    "df_test=predict_txt(\"toxicwords.txt\", df_test)\n",
    "\n",
    "def f1(predictions, gold):\n",
    "    if len(gold) == 0:\n",
    "        return 1.0 if len(predictions) == 0 else 0.0\n",
    "    if len(predictions) == 0:\n",
    "        return 0.0\n",
    "    predictions_set = set(predictions)\n",
    "    gold_list = ast.literal_eval(gold)\n",
    "    gold_set = set(gold_list)\n",
    "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
    "    denom = len(predictions_set) + len(gold_set)\n",
    "    return float(nom) / float(denom)\n",
    "\n",
    "\n",
    "\n",
    "predicted = df_test[\"pindex\"].tolist()\n",
    "actual = df_test[\"index\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "f1_scores = []\n",
    "for pred, act in zip(predicted, actual):\n",
    "    f1_scores.append(f1(pred, act))\n",
    "\n",
    "macro_avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(\"Macro-averaged F1-score:\", macro_avg_f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-------------------------------------2. TXT----------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged F1-score: 0.38196479337490424\n"
     ]
    }
   ],
   "source": [
    "#TOXICWORDSLONG.TXT\n",
    "\n",
    "df_test=predict_txt(\"toxicwordslong.txt\", df_test)\n",
    "\n",
    "\n",
    "predicted = df_test[\"pindex\"].tolist()\n",
    "actual = df_test[\"index\"].tolist()\n",
    "\n",
    "f1_scores = []\n",
    "for pred, act in zip(predicted, actual):\n",
    "    f1_scores.append(f1(pred, act))\n",
    "\n",
    "macro_avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(\"Macro-averaged F1-score:\", macro_avg_f1)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------------------------- 3. teszt -----------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Az eltávolított sorok száma: 0\n"
     ]
    }
   ],
   "source": [
    "def delete_duplicate_lines(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    unique_lines = list(set(lines))\n",
    "    num_deleted_lines = len(lines) - len(unique_lines)\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        for line in unique_lines:\n",
    "            file.write(line)\n",
    "\n",
    "    return num_deleted_lines\n",
    "\n",
    "filename = 'tw2.txt'\n",
    "\n",
    "deleted_lines = delete_duplicate_lines(filename)\n",
    "print(f\"Az eltávolított sorok száma: {deleted_lines}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged F1-score: 0.49474405574385877\n"
     ]
    }
   ],
   "source": [
    "#TW2.TXT\n",
    "\n",
    "df_test=predict_txt(\"tw2.txt\", df_test)\n",
    "\n",
    "\n",
    "predicted = df_test[\"pindex\"].tolist()\n",
    "actual = df_test[\"index\"].tolist()\n",
    "\n",
    "f1_scores = []\n",
    "for pred, act in zip(predicted, actual):\n",
    "    f1_scores.append(f1(pred, act))\n",
    "\n",
    "macro_avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(\"Macro-averaged F1-score:\", macro_avg_f1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------------------------------BERT MODELL--------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x11698f5f0>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "df_2=pd.read_csv('data.csv')\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def toxic_tokens(df):\n",
    "    for index, sample in df.iterrows():\n",
    "        spans = eval(sample['spans'])\n",
    "        correct_spans = spans.copy()\n",
    "        chars = list(sample['text'])\n",
    "        for i, char in enumerate(chars):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if (i in spans) and (i - 1 not in spans) and (chars[i - 1].isalnum()) and (char.isalnum()):\n",
    "                correct_spans.append(i - 1)\n",
    "            elif (i - 1 in spans) and (i not in spans) and (chars[i - 1].isalnum()) and (char.isalnum()):\n",
    "                correct_spans.append(i)\n",
    "        correct_spans.sort()\n",
    "        sample['spans'] = correct_spans\n",
    "\n",
    "    df['toxic_tokens'] = [list() for x in range(len(df.index))]\n",
    "    for _, sample in df_2.iterrows():\n",
    "            toxic = ''\n",
    "            for i, char in enumerate(list(sample[\"text\"])):\n",
    "                if i in sample[\"spans\"]:\n",
    "                    toxic += char\n",
    "                elif len(toxic):\n",
    "                    sample['toxic_tokens'].append(toxic)\n",
    "                    toxic = ''\n",
    "            if toxic:\n",
    "                sample['toxic_tokens'].append(toxic)\n",
    "    return df\n",
    "\n",
    "df_2=toxic_tokens(df_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def ranges(nums):\n",
    "    nums = sorted(set(nums))\n",
    "    gaps = [[s, e] for s, e in zip(nums, nums[1:]) if s + 1 < e]\n",
    "    edges = iter(nums[:1] + sum(gaps, []) + nums[-1:])\n",
    "    return list(zip(edges, edges))\n",
    "\n",
    "def create_token_labels(df):\n",
    "    text = nlp(df['text'])\n",
    "    token_start = [token.idx for token in text]\n",
    "    token_end = [token.idx + len(token) - 1 for token in text]\n",
    "    toxic_ranges = ranges(df['spans'])\n",
    "    l = len(df['text'])\n",
    "    for range in toxic_ranges:\n",
    "        start, end = range\n",
    "        if end >= l:\n",
    "            end = l - 1\n",
    "        while start < l and df['text'][start] == ' ':\n",
    "            start += 1\n",
    "        while end >= 0 and df['text'][end] == ' ':\n",
    "            end -= 1\n",
    "        start = token_start[bisect_right(token_start, start) - 1]\n",
    "        end = token_end[bisect_left(token_end, end)]\n",
    "        if start >= end:\n",
    "            print('Error:', df['text'])\n",
    "            continue\n",
    "        token_span = text.char_span(start, end + 1)\n",
    "        for token in token_span:\n",
    "                token.ent_type_ = 'toxic'\n",
    "\n",
    "    bert_tokens = []\n",
    "    token_labels = []\n",
    "    for token in text:\n",
    "        bert_subtokens = tokenizer.tokenize(token.text)\n",
    "        bert_tokens += bert_subtokens\n",
    "        token_labels += [int(token.ent_type_ == 'toxic') for _ in bert_subtokens]\n",
    "\n",
    "    return bert_tokens, token_labels\n",
    "\n",
    "\n",
    "def bert_token(df):\n",
    "    bert_tokens_list = []\n",
    "    token_labels_list = []\n",
    "\n",
    "    for _, sample in df.iterrows():\n",
    "        bert_tokens, token_labels = create_token_labels(sample)\n",
    "        bert_tokens_list.append(bert_tokens)\n",
    "        token_labels_list.append(token_labels)\n",
    "\n",
    "    df['bert_tokens'] = bert_tokens_list\n",
    "    df['token_labels'] = token_labels_list\n",
    "\n",
    "    return df\n",
    "\n",
    "df_2 = bert_token(df_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "df_test2=df_test\n",
    "df_test2['bert_tokens'] = [list() for x in range(len(df_test2.index))]\n",
    "\n",
    "for _, sample in df_test2.iterrows():\n",
    "    text = nlp(sample['text'])\n",
    "    for token in text:\n",
    "        sample['bert_tokens'] += tokenizer.tokenize(token.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(7939, 2000)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_train = max(len(x) for x in df_2['bert_tokens'])\n",
    "maxlen_test = max(len(x) for x in df_test2['bert_tokens'])\n",
    "maxlen = max(maxlen_train, maxlen_test)\n",
    "\n",
    "train_tokens = [['[CLS]'] + t[:maxlen - 2] + ['[SEP]'] for t in df_2['bert_tokens']]\n",
    "test_tokens = [['[CLS]'] + t[:maxlen - 2] + ['[SEP]'] for t in df_test2['bert_tokens']]\n",
    "\n",
    "len(train_tokens), len(test_tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 497/497 [2:51:49<00:00, 20.74s/it]  \n",
      "100%|██████████| 497/497 [1:48:22<00:00, 13.08s/it]    \n",
      "100%|██████████| 125/125 [06:24<00:00,  3.07s/it]\n"
     ]
    }
   ],
   "source": [
    "def padding(tokens, max_len=maxlen):\n",
    "    tokens_len = len(tokens)\n",
    "    pad_len = max(0, max_len - tokens_len)\n",
    "    return (\n",
    "        pad_sequences([tokens], maxlen=max_len, truncating=\"post\", padding=\"post\", dtype=\"int\"),\n",
    "        np.concatenate([np.ones(tokens_len, dtype=\"int\"), np.zeros(pad_len, dtype=\"int\")], axis=0)\n",
    "    )\n",
    "\n",
    "def token_and_mask(tokens):\n",
    "    token_ids, masks = [], []\n",
    "\n",
    "    for x in tokens:\n",
    "        token_id, mask = padding(tokenizer.convert_tokens_to_ids(x))\n",
    "        token_ids.append(token_id[0])\n",
    "        masks.append(mask)\n",
    "\n",
    "    token_ids = np.array(token_ids)\n",
    "    masks = np.array(masks)\n",
    "\n",
    "    return token_ids, masks\n",
    "\n",
    "train_token_ids, train_masks = token_and_mask(train_tokens)\n",
    "test_token_ids, test_masks = token_and_mask(test_tokens)\n",
    "\n",
    "\n",
    "train_token_labels = list(map(lambda t: [0] + t[:maxlen - 2] + [0], df_2['token_labels']))\n",
    "\n",
    "train_y = pad_sequences(train_token_labels, maxlen=maxlen, truncating=\"post\", padding=\"post\")[:, :, None]\n",
    "train_y.shape, np.mean(train_y)\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.hidden = nn.Linear(bert.config.hidden_size, 64)\n",
    "        self.hidden_activation = nn.LeakyReLU(0.1)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs[0]\n",
    "        cls_output = self.hidden(cls_output)\n",
    "        cls_output = self.hidden_activation(cls_output)\n",
    "        cls_output = self.output(cls_output)\n",
    "        cls_output = self.output_activation(cls_output)\n",
    "        criterion = nn.BCELoss()\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = criterion(cls_output, labels.float())\n",
    "        return loss, cls_output\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = BertClassifier(BertModel.from_pretrained('bert-base-uncased')).to(device)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_token_ids), torch.tensor(train_masks), torch.tensor(train_y))\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(test_token_ids), torch.tensor(test_masks))\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=3e-6)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "EPOCHS = 2\n",
    "loss = nn.BCELoss()\n",
    "total_len = len(train_token_ids)\n",
    "batch_losses = []\n",
    "model.train()\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(tqdm(train_dataloader)):\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "        loss, _ = model(input_ids=token_ids, attention_mask=masks, labels=labels)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(train_loss / (step_num + 1))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_tokens = []\n",
    "test_attention_masks = []\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids, masks = tuple(t.to(device) for t in batch_data)\n",
    "        _, output = model(input_ids=token_ids, attention_mask=masks)\n",
    "        test_tokens += token_ids.tolist()\n",
    "        test_attention_masks += masks.tolist()\n",
    "        test_preds += output[:, :, 0].tolist()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def f1_scoremetric(predictions, gold, n):\n",
    "    predictions = set(predictions)\n",
    "    gold = set(gold)\n",
    "\n",
    "    if len(gold) == 0:\n",
    "        return {\n",
    "            'f1': 1 if len(predictions) == 0 else 0\n",
    "        }\n",
    "\n",
    "    nom = 2 * len(predictions.intersection(gold))\n",
    "    denom = len(predictions) + len(gold)\n",
    "\n",
    "    if len(gold) == n:\n",
    "        return {\n",
    "            'f1': nom / denom\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'f1': nom / denom\n",
    "    }\n",
    "\n",
    "\n",
    "def clean_f1(threshold):\n",
    "    n = len(test_tokens)\n",
    "    y_pred = []\n",
    "    all_spans = []\n",
    "    f1=0\n",
    "\n",
    "    for i in range(n):\n",
    "        reconstructed_text = ''\n",
    "        spans = []\n",
    "        original_text = df_test2.iloc[i]['text']\n",
    "        original_text = original_text.replace('É', 'e')\n",
    "        original_text = original_text.replace('\\u200b', '')\n",
    "        original_text = original_text.replace('ü', 'u')\n",
    "        original_l = len(original_text)\n",
    "        idx = 0\n",
    "        tokens = test_tokens[i]\n",
    "        n_tokens = len(tokens)\n",
    "\n",
    "        j = 1\n",
    "        last_token = None\n",
    "        prev_token_toxic = False\n",
    "        prev_token_prob = 0\n",
    "        prev_idx = -1\n",
    "\n",
    "        while j < n_tokens:\n",
    "            while idx < original_l and original_text[idx].isspace():\n",
    "                reconstructed_text += original_text[idx]\n",
    "                idx += 1\n",
    "\n",
    "            word = tokens[j]\n",
    "\n",
    "            if word == '[SEP]':\n",
    "                last_token = j - 1\n",
    "                break\n",
    "\n",
    "            if ord(original_text[idx]) == 65039:\n",
    "                print('Problematic char at', idx, i)\n",
    "                reconstructed_text += original_text[idx]\n",
    "                idx += 1\n",
    "\n",
    "            if word == '[UNK]':\n",
    "                reconstructed_text += original_text[idx]\n",
    "                idx += 1\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "            max_toxic_prob = test_preds[i][j]\n",
    "            print(tokens)\n",
    "            while j < n_tokens - 1 and tokens[j + 1].startswith('##'):\n",
    "                j += 1\n",
    "                word += tokens[j][2:]\n",
    "                max_toxic_prob = max(max_toxic_prob, test_preds[i][j])\n",
    "\n",
    "            word_l = len(word)\n",
    "            y_pred += [min(prev_token_prob, max_toxic_prob) for _ in range(prev_idx + 1, idx)]\n",
    "\n",
    "            if min(prev_token_prob, max_toxic_prob) >= threshold:\n",
    "                spans += list(range(prev_idx + 1, idx))\n",
    "\n",
    "            y_pred += [max_toxic_prob for _ in range(word_l)]\n",
    "            is_toxic = (max_toxic_prob >= threshold)\n",
    "            prev_token_prob = max_toxic_prob\n",
    "\n",
    "            if word == original_text[idx: idx + word_l].lower():\n",
    "                reconstructed_text += word\n",
    "\n",
    "                if is_toxic:\n",
    "                    spans += list(range(idx, idx + word_l))\n",
    "\n",
    "                idx += word_l\n",
    "                prev_idx = idx - 1\n",
    "                prev_token_toxic = is_toxic\n",
    "            else:\n",
    "                print(word, original_text[idx: idx + word_l])\n",
    "\n",
    "            j += 1\n",
    "\n",
    "        while idx < original_l and original_text[idx].isspace():\n",
    "            reconstructed_text += original_text[idx]\n",
    "            idx += 1\n",
    "\n",
    "        y_pred += [prev_token_prob for _ in range(prev_idx + 1, idx)]\n",
    "\n",
    "        if reconstructed_text != original_text.lower():\n",
    "            print('ISSUE', i)\n",
    "        else:\n",
    "            metrics = f1_scoremetric(spans, eval(df_test2.iloc[i]['index']), len(original_text))\n",
    "            f1 += metrics['f1']\n",
    "\n",
    "\n",
    "        all_spans.append(spans)\n",
    "\n",
    "    return f1 / n, all_spans\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "thresholds = [0.01 * x for x in range(100)]\n",
    "f1_scores=[]\n",
    "for x in tqdm(thresholds):\n",
    "    scores = clean_f1(x)\n",
    "    f1_scores.append(scores[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test2[\"predicted\"]=\"\"\n",
    "df_test2[\"predicted\"] = [text_scores for uid, text_scores in zip(range(len(test_tokens)), clean_f1(0.5)[1])]\n",
    "df_test2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged F1-score: 0.6008183241650797\n"
     ]
    }
   ],
   "source": [
    "predicted = df_test2[\"predicted\"].tolist()\n",
    "actual = df_test2[\"index\"].tolist()\n",
    "\n",
    "f1_scores2 = []\n",
    "for pred, act in zip(predicted, actual):\n",
    "    f1_scores2.append(f1(pred, act))\n",
    "\n",
    "macro_avg_f1 = np.mean(f1_scores2)\n",
    "\n",
    "print(\"Macro-averaged F1-score:\", macro_avg_f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "with open('predictions.txt', 'w') as out:\n",
    "    for uid, text_scores in zip(range(len(test_tokens)), clean_f1(0.5)[3]):\n",
    "        out.write(f'{str(uid)}\\t{str(text_scores)}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------------------------BERT MODELL 5 EPOCH------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 497/497 [1:41:29<00:00, 12.25s/it]\n",
      "100%|██████████| 497/497 [1:39:29<00:00, 12.01s/it]\n",
      "100%|██████████| 497/497 [1:40:29<00:00, 12.13s/it]\n",
      "100%|██████████| 497/497 [1:41:22<00:00, 12.24s/it]\n",
      "100%|██████████| 497/497 [1:41:57<00:00, 12.31s/it]\n",
      "100%|██████████| 125/125 [05:42<00:00,  2.74s/it]\n"
     ]
    }
   ],
   "source": [
    "def padding(tokens, max_len=maxlen):\n",
    "    tokens_len = len(tokens)\n",
    "    pad_len = max(0, max_len - tokens_len)\n",
    "    return (\n",
    "        pad_sequences([tokens], maxlen=max_len, truncating=\"post\", padding=\"post\", dtype=\"int\"),\n",
    "        np.concatenate([np.ones(tokens_len, dtype=\"int\"), np.zeros(pad_len, dtype=\"int\")], axis=0)\n",
    "    )\n",
    "\n",
    "def token_and_mask(tokens):\n",
    "    token_ids, masks = [], []\n",
    "\n",
    "    for x in tokens:\n",
    "        token_id, mask = padding(tokenizer.convert_tokens_to_ids(x))\n",
    "        token_ids.append(token_id[0])\n",
    "        masks.append(mask)\n",
    "\n",
    "    token_ids = np.array(token_ids)\n",
    "    masks = np.array(masks)\n",
    "\n",
    "    return token_ids, masks\n",
    "\n",
    "train_token_ids, train_masks = token_and_mask(train_tokens)\n",
    "test_token_ids, test_masks = token_and_mask(test_tokens)\n",
    "\n",
    "\n",
    "train_token_labels = list(map(lambda t: [0] + t[:maxlen - 2] + [0], df_2['token_labels']))\n",
    "\n",
    "train_y = pad_sequences(train_token_labels, maxlen=maxlen, truncating=\"post\", padding=\"post\")[:, :, None]\n",
    "train_y.shape, np.mean(train_y)\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.hidden = nn.Linear(bert.config.hidden_size, 64)\n",
    "        self.hidden_activation = nn.LeakyReLU(0.1)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs[0]\n",
    "        cls_output = self.hidden(cls_output)\n",
    "        cls_output = self.hidden_activation(cls_output)\n",
    "        cls_output = self.output(cls_output)\n",
    "        cls_output = self.output_activation(cls_output)\n",
    "        criterion = nn.BCELoss()\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = criterion(cls_output, labels.float())\n",
    "        return loss, cls_output\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = BertClassifier(BertModel.from_pretrained('bert-base-uncased')).to(device)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_token_ids), torch.tensor(train_masks), torch.tensor(train_y))\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(test_token_ids), torch.tensor(test_masks))\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=3e-6)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "EPOCHS = 5\n",
    "loss = nn.BCELoss()\n",
    "total_len = len(train_token_ids)\n",
    "batch_losses = []\n",
    "model.train()\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(tqdm(train_dataloader)):\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "        loss, _ = model(input_ids=token_ids, attention_mask=masks, labels=labels)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(train_loss / (step_num + 1))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_tokens = []\n",
    "test_attention_masks = []\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids, masks = tuple(t.to(device) for t in batch_data)\n",
    "        _, output = model(input_ids=token_ids, attention_mask=masks)\n",
    "        test_tokens += token_ids.tolist()\n",
    "        test_attention_masks += masks.tolist()\n",
    "        test_preds += output[:, :, 0].tolist()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def f1_scoremetric(predictions, gold, n):\n",
    "    predictions = set(predictions)\n",
    "    gold = set(gold)\n",
    "    if len(gold) == 0:\n",
    "        return {\n",
    "            'f1': 1 if len(predictions)==0 else 0\n",
    "        }\n",
    "    nom = 2*len(predictions.intersection(gold))\n",
    "    denom = len(predictions)+len(gold)\n",
    "    if len(gold) == n:\n",
    "        return {\n",
    "            'f1': nom/denom,\n",
    "        }\n",
    "    return {\n",
    "        'f1': nom/denom,\n",
    "    }\n",
    "\n",
    "def clean_f1(threshold):\n",
    "    n = len(test_tokens)\n",
    "    y_pred = []\n",
    "    all_spans = []\n",
    "    f1=0\n",
    "    for i in range(n):\n",
    "        reconstructed_text = ''\n",
    "        spans = []\n",
    "        original_text = df_test2.iloc[i]['text']\n",
    "        original_text = original_text.replace('É', 'e')\n",
    "        original_text = original_text.replace('\\u200b', '')\n",
    "        original_text = original_text.replace('ü', 'u')\n",
    "        original_l = len(original_text)\n",
    "        idx = 0\n",
    "        tokens = test_tokens[i]\n",
    "        n_tokens = len(tokens)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "        j = 1\n",
    "        last_token = None\n",
    "        prev_token_toxic = False\n",
    "        prev_token_prob = 0\n",
    "        prev_idx = -1\n",
    "        while j < n_tokens:\n",
    "            while idx < original_l and original_text[idx].isspace():\n",
    "                reconstructed_text += original_text[idx]\n",
    "                idx += 1\n",
    "            word = tokens[j]\n",
    "            if word == '[SEP]':\n",
    "                last_token = j - 1\n",
    "                break\n",
    "            if ord(original_text[idx]) == 65039:\n",
    "                print('Problematic char at', idx, i)\n",
    "                reconstructed_text += original_text[idx]\n",
    "                idx += 1\n",
    "            if word == '[UNK]':\n",
    "                reconstructed_text += original_text[idx]\n",
    "                idx += 1\n",
    "                j += 1\n",
    "                continue\n",
    "            max_toxic_prob = test_preds[i][j]\n",
    "            while j < n_tokens - 1 and tokens[j + 1].startswith('##'):\n",
    "                j += 1\n",
    "                word += tokens[j][2:]\n",
    "                max_toxic_prob = max(max_toxic_prob, test_preds[i][j])\n",
    "            word_l = len(word)\n",
    "            y_pred += [min(prev_token_prob, max_toxic_prob) for _ in range(prev_idx + 1, idx)]\n",
    "            if min(prev_token_prob, max_toxic_prob) >= threshold:\n",
    "                spans += list(range(prev_idx + 1, idx))\n",
    "            y_pred += [max_toxic_prob for _ in range(word_l)]\n",
    "            is_toxic = (max_toxic_prob >= threshold)\n",
    "            prev_token_prob = max_toxic_prob\n",
    "            if word == original_text[idx: idx + word_l].lower():\n",
    "                reconstructed_text += word\n",
    "                if is_toxic:\n",
    "                    spans += list(range(idx, idx + word_l))\n",
    "                idx += word_l\n",
    "                prev_idx = idx - 1\n",
    "                prev_token_toxic = is_toxic\n",
    "            else:\n",
    "                print(word, original_text[idx: idx + word_l])\n",
    "            j += 1\n",
    "\n",
    "        while idx < original_l and original_text[idx].isspace():\n",
    "            reconstructed_text += original_text[idx]\n",
    "            idx += 1\n",
    "        y_pred += [prev_token_prob for _ in range(prev_idx + 1, idx)]\n",
    "\n",
    "        if reconstructed_text != original_text.lower():\n",
    "            print('ISSUE', i)\n",
    "        else:\n",
    "            metrics = f1_scoremetric(spans, eval(df_test2.iloc[i]['index']), len(original_text))\n",
    "            f1 += metrics['f1']\n",
    "\n",
    "        all_spans.append(spans)\n",
    "\n",
    "    return f1 / n, all_spans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:12<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.01 * x for x in range(100)]\n",
    "f1_scores=[]\n",
    "for x in tqdm(thresholds):\n",
    "    scores = clean_f1(x)\n",
    "    f1_scores.append(scores[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  index  \\\n0     [84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...   \n1                              [81, 82, 83, 84, 85, 86]   \n2                                                    []   \n3                                                    []   \n4                                                    []   \n...                                                 ...   \n1995  [4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...   \n1996                               [23, 24, 25, 26, 27]   \n1997  [157, 158, 159, 160, 161, 162, 163, 164, 165, ...   \n1998                                                 []   \n1999                               [828, 829, 830, 831]   \n\n                                                   text  toxic  \\\n0     That's right. They are not normal. And I am st...      0   \n1     \"Watch people die from taking away their healt...      0   \n2     tens years ago i contacted the PDR and suggest...      0   \n3     The parallels between the ANC and the Sicilian...      0   \n4     Intel Community: ‘How can we work for a Presid...      0   \n...                                                 ...    ...   \n1995  hey loser change your name to something more a...      0   \n1996  And you are a complete moron who obviously doe...      0   \n1997  Such vitriol from the left.  Who would have th...      0   \n1998  It is now time for most of you to expand your ...      0   \n1999  Why does this author think she can demand, or ...      0   \n\n                                            bert_tokens  \\\n0     [that, ', s, right, ., they, are, not, normal,...   \n1     [\", watch, people, die, from, taking, away, th...   \n2     [tens, years, ago, i, contacted, the, pd, ##r,...   \n3     [the, parallels, between, the, an, ##c, and, t...   \n4     [intel, community, :, ‘, how, can, we, work, f...   \n...                                                 ...   \n1995  [hey, loser, change, your, name, to, something...   \n1996  [and, you, are, a, complete, mor, ##on, who, o...   \n1997  [such, vi, ##tri, ##ol, from, the, left, ., wh...   \n1998  [it, is, now, time, for, most, of, you, to, ex...   \n1999  [why, does, this, author, think, she, can, dem...   \n\n                                              predicted  \n0     [118, 119, 120, 121, 122, 123, 133, 134, 135, ...  \n1                              [81, 82, 83, 84, 85, 86]  \n2              [483, 484, 485, 486, 487, 488, 489, 490]  \n3              [413, 414, 415, 416, 417, 418, 419, 420]  \n4                   [663, 664, 665, 666, 667, 668, 669]  \n...                                                 ...  \n1995    [4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77]  \n1996  [23, 24, 25, 26, 27, 80, 81, 82, 83, 84, 85, 8...  \n1997  [157, 158, 159, 160, 161, 162, 163, 164, 165, ...  \n1998                                                 []  \n1999  [109, 110, 111, 112, 828, 829, 830, 831, 837, ...  \n\n[2000 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>text</th>\n      <th>toxic</th>\n      <th>bert_tokens</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...</td>\n      <td>That's right. They are not normal. And I am st...</td>\n      <td>0</td>\n      <td>[that, ', s, right, ., they, are, not, normal,...</td>\n      <td>[118, 119, 120, 121, 122, 123, 133, 134, 135, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[81, 82, 83, 84, 85, 86]</td>\n      <td>\"Watch people die from taking away their healt...</td>\n      <td>0</td>\n      <td>[\", watch, people, die, from, taking, away, th...</td>\n      <td>[81, 82, 83, 84, 85, 86]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[]</td>\n      <td>tens years ago i contacted the PDR and suggest...</td>\n      <td>0</td>\n      <td>[tens, years, ago, i, contacted, the, pd, ##r,...</td>\n      <td>[483, 484, 485, 486, 487, 488, 489, 490]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[]</td>\n      <td>The parallels between the ANC and the Sicilian...</td>\n      <td>0</td>\n      <td>[the, parallels, between, the, an, ##c, and, t...</td>\n      <td>[413, 414, 415, 416, 417, 418, 419, 420]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[]</td>\n      <td>Intel Community: ‘How can we work for a Presid...</td>\n      <td>0</td>\n      <td>[intel, community, :, ‘, how, can, we, work, f...</td>\n      <td>[663, 664, 665, 666, 667, 668, 669]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>[4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...</td>\n      <td>hey loser change your name to something more a...</td>\n      <td>0</td>\n      <td>[hey, loser, change, your, name, to, something...</td>\n      <td>[4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77]</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>[23, 24, 25, 26, 27]</td>\n      <td>And you are a complete moron who obviously doe...</td>\n      <td>0</td>\n      <td>[and, you, are, a, complete, mor, ##on, who, o...</td>\n      <td>[23, 24, 25, 26, 27, 80, 81, 82, 83, 84, 85, 8...</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>[157, 158, 159, 160, 161, 162, 163, 164, 165, ...</td>\n      <td>Such vitriol from the left.  Who would have th...</td>\n      <td>0</td>\n      <td>[such, vi, ##tri, ##ol, from, the, left, ., wh...</td>\n      <td>[157, 158, 159, 160, 161, 162, 163, 164, 165, ...</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>[]</td>\n      <td>It is now time for most of you to expand your ...</td>\n      <td>0</td>\n      <td>[it, is, now, time, for, most, of, you, to, ex...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>[828, 829, 830, 831]</td>\n      <td>Why does this author think she can demand, or ...</td>\n      <td>0</td>\n      <td>[why, does, this, author, think, she, can, dem...</td>\n      <td>[109, 110, 111, 112, 828, 829, 830, 831, 837, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test2[\"predicted\"]=\"\"\n",
    "df_test2[\"predicted\"] = [text_scores for uid, text_scores in zip(range(len(test_tokens)), clean_f1(0.57)[1])] #0.57\n",
    "df_test2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged F1-score: 0.6414839024042502\n",
      "F1 score: 64.15%\n"
     ]
    }
   ],
   "source": [
    "predicted = df_test2[\"predicted\"].tolist()\n",
    "actual = df_test2[\"index\"].tolist()\n",
    "\n",
    "f1_scores2 = []\n",
    "for pred, act in zip(predicted, actual):\n",
    "    f1_scores2.append(f1(pred, act))\n",
    "\n",
    "macro_avg_f1 = np.mean(f1_scores2)\n",
    "\n",
    "print(\"Macro-averaged F1-score:\", macro_avg_f1)\n",
    "txt = \"F1 score: {F1_score:.2f}%\"\n",
    "print(txt.format(F1_score = macro_avg_f1*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}